#chainer good bc of ACER
#https://github.com/chainer/chainerrl
import numpy as np

import gym
import malware_rl

import chainer
import chainer.functions as F
import chainer.links as L
import chainerrl 

from chainerrl.action_value import DiscreteActionValue
from chainerrl import links
from chainerrl.agents import acer
from chainerrl.distribution import SoftmaxDistribution
from chainerrl import misc
from chainerrl.optimizers import rmsprop_async
from chainerrl import policies
from chainerrl import q_functions
from chainerrl.replay_buffer import EpisodicReplayBuffer
from chainerrl import v_functions
from chainerrl.initializers import LeCunNormal


#creates an ACER agent
def create_acer_agent(env):
    #our observation space dimension of malware
    obs_dim = env.observation_space.shape[0]
    #the list of actions that we can perform on the malware
    n_actions = env.action_space.n
    #our acer network
    #consists of pi (our policy) and our q (our q function)
    model = acer.ACERSeparateModel(
        pi=links.Sequence(
            L.Linear( obs_dim, 1024, initialW=LeCunNormal(1e-3)),
            F.relu,
            L.Linear( 1024, 512, initialW=LeCunNormal(1e-3)),
            F.relu,
            L.Linear( 512, n_actions, initialW=LeCunNormal(1e-3)),
            SoftmaxDistribution),
        q=links.Sequence(
            L.Linear( obs_dim, 1024, initialW=LeCunNormal(1e-3)),
            F.relu,
            L.Linear( 1024, 512, initialW=LeCunNormal(1e-3)),
            F.relu,
            L.Linear( 512, n_actions, initialW=LeCunNormal(1e-3)),
            DiscreteActionValue),
        )
    #optimizer for the acer 
    opt = rmsprop_async.RMSpropAsync( lr=7e-4, eps=1e-2, alpha=0.99)
    opt.setup( model )
    #hook to the chainer model
    opt.add_hook( chainer.optimizer.GradientClipping(40) )

    replay_buffer = EpisodicReplayBuffer( 128 )
    #the agent itself, params from original file
    agent = acer.ACER( model, opt, 
        gamma=0.95, # reward discount factor
        t_max=32, # update the model after this many local steps
        replay_buffer=replay_buffer,
        n_times_replay=4, # number of times experience replay is repeated for each update
        replay_start_size=64, # don't start replay unless we have this many experiences in the buffer
        disable_online_update=True, # rely only on experience buffer
        use_trust_region=True,  # enable trust region policy optimiztion
        trust_region_delta=0.1,  # a parameter for TRPO
        truncation_threshold=5.0, # truncate large importance weights
        beta=1e-2, # entropy regularization parameter
        phi= lambda obs: obs.astype(np.float32, copy=False) )

    return agent


class QFunction(chainer.Chain):

    def __init__(self, obs_size, n_actions, n_hidden_channels=[1024,256]):
        super(QFunction,self).__init__()
        net = []
        inpdim = obs_size
        for i,n_hid in enumerate(n_hidden_channels):
            net += [ ('l{}'.format(i), L.Linear( inpdim, n_hid ) ) ]
            net += [ ('norm{}'.format(i), L.BatchNormalization( n_hid ) ) ]
            net += [ ('_act{}'.format(i), F.relu ) ]
            inpdim = n_hid

        net += [('output', L.Linear( inpdim, n_actions) )]

        with self.init_scope():
            for n in net:
                if not n[0].startswith('_'):
                    setattr(self, n[0], n[1])

        self.forward = net


    def __call__(self, x, test=False):
        """
        Args:
            x (ndarray or chainer.Variable): An observation
            test (bool): a flag indicating whether it is in test mode
        """
        for n, f in self.forward:
            if not n.startswith('_'):
                x = getattr(self, n)(x)
            else:
                x = f(x)

        return chainerrl.action_value.DiscreteActionValue(x)

def create_ddqn_agent(env):

    obs_dim = env.observation_space.shape[0]
    n_actions = env.action_space.n

    q_func = QFunction(obs_dim, n_actions)

    optimizer = chainer.optimizers.Adam(eps=1e-2)
    optimizer.setup(q_func)

    # Set the discount factor that discounts future rewards.
    gamma = 0.95

    # Use epsilon-greedy for exploration
    explorer = chainerrl.explorers.Boltzmann()

    # DQN uses Experience Replay.
    # Specify a replay buffer and its capacity.
    replay_buffer = chainerrl.replay_buffer.ReplayBuffer(capacity=1000)

    # Chainer only accepts numpy.float32 by default, make sure
    # a converter as a feature extractor function phi.
    phi = lambda x: x.astype(np.float32, copy=False)

    # Now create an agent that will interact with the environment.
    # DQN agent as described in Mnih (2013) and Mnih (2015).
    # http://arxiv.org/pdf/1312.5602.pdf
    # http://arxiv.org/abs/1509.06461        
    agent = chainerrl.agents.DoubleDQN(
        q_func, optimizer, replay_buffer, gamma, explorer,
        replay_start_size=32, update_interval=1,
        target_update_interval=100, phi=phi)

    return agent
import os
def get_latest_model_from(basedir):
    dirs = os.listdir(basedir)
    lastmodel = -1
    for d in dirs:
        try:
            if int(d) > lastmodel:
                lastmodel = int(d)
        except ValueError:
            continue

    assert lastmodel >= 0, "No saved models!"
    return os.path.join(basedir, str(lastmodel))


from malware_rl.envs.utils import interface, malconv
from malware_rl.envs.controls import modifier
import random
ACTION_LOOKUP = {
    i: act for i, act in enumerate(modifier.ACTION_TABLE.keys())
}
#training the ACER agent
def train_agent(rounds=10000, use_score=False, name='result_dir', test_set = "/home/jovyan/Research/malware_rl/sets/test_set", train_set = "/home/jovyan/Research/malware_rl/sets/train_set", create_agent=create_acer_agent, gym_env = "malconv-train-v0"):
    if(name == "random"):
        mc = malconv.MalConv()
        sha256_train  = interface.get_available_sha256('/home/jovyan/Research/h4rm0ny/malconv/rl_train_exp.csv')[:5]
        if not os.path.exists(train_set):
            os.makedirs(train_set)
        for s in sha256_train:
            action = random.choice(ACTION_LOOKUP)
            bytez = interface.fetch_file(s)
            bytez = modifier.modify_sample(bytez, action)

            evade_path = os.path.join( test_set, os.path.basename(s))
            with open(evade_path, 'wb') as out:
                out.write(bytez)
            if not os.path.exists(train_set):
                os.makedirs(train_set)  
        sha256_test  = interface.get_available_sha256('/home/jovyan/Research/h4rm0ny/malconv/rl_test_exp.csv')[:5]
        for s in sha256_test:
            action = random.choice(ACTION_LOOKUP)
            bytez = interface.fetch_file(s)
            bytez = modifier.modify_sample(bytez, action)

            evade_path = os.path.join( test_set, os.path.basename(s))
            with open(evade_path, 'wb') as out:
                out.write(bytez)
        return 1
    #we are training on the malconv gym
    env = gym.make( gym_env ) 
    #setting random seeds so we can reproduce results
    np.random.seed(41)
    env.seed(41)
    #creating our agent
    agent = create_agent(env)
    #run through training, evaluate and give reward based on outcome
    
    
    
    
    chainerrl.experiments.train_agent_with_evaluation(
        agent, env,
        steps=rounds,                   # Train the agent for this many rounds steps
        train_max_episode_len=601,     # Maximum length of each episodes        
        eval_interval=10,             # Evaluate the agent after every step
        eval_n_episodes = 10,         #eval every episode
        eval_n_steps = None,
        save_best_so_far_agent = False,
        outdir=name)                    # Save everything to 'result' directory
    
    mc = malconv.MalConv()
    sha256_train  = interface.get_available_sha256('/home/jovyan/Research/h4rm0ny/malconv/rl_train_exp.csv')[:5]
    if not os.path.exists(train_set):
        os.makedirs(train_set)
    for s in sha256_train:
        mal = np.array(mc.extract(interface.fetch_file(s)))
        action = ACTION_LOOKUP[agent.act(mal)]
        bytez = interface.fetch_file(s)
        bytez = modifier.modify_sample(bytez, action)

        evade_path = os.path.join( test_set, os.path.basename(s))
        with open(evade_path, 'wb') as out:
            out.write(bytez)
    if not os.path.exists(train_set):
        os.makedirs(train_set)  
    sha256_test  = interface.get_available_sha256('/home/jovyan/Research/h4rm0ny/malconv/rl_test_exp.csv')[:5]
    for s in sha256_test:
        mal = np.array(mc.extract(interface.fetch_file(s)))
        action = ACTION_LOOKUP[agent.act(mal)]
        bytez = interface.fetch_file(s)
        bytez = modifier.modify_sample(bytez, action)

        evade_path = os.path.join( test_set, os.path.basename(s))
        with open(evade_path, 'wb') as out:
            out.write(bytez)



#training the ACER agent
def test(rounds=10, use_score=False, name='result_dir', create_agent=create_acer_agent, gym_env = "malconv-train-v0"):
    #we are training on the malconv gym
    env = gym.make( gym_env ) 
    #setting random seeds so we can reproduce results
    np.random.seed(42)
    env.seed(42)
    #creating our agent
    agent = create_agent(env)
    # pull latest stored model
    last_model_dir = get_latest_model_from('models/')
    agent.load( last_model_dir )
    chainerrl.experiments.collect_demonstrations(agent,env,steps = rounds,episodes = 1,outdir = name)
    
                 # Save everything to 'result' directory


if __name__ == '__main__':
    print("We go")
    agent_score = train_agent(rounds=50000, use_score=True, name='models/', create_agent=create_acer_agent) # allow agent to see scores
    # models are automatically saved
    print("done score")
    #use this model if you want to see if the RL can learn against a black box model
    agent_blackbox = train_agent( rounds=50000, use_score=False, name='models/acer_chainer', create_agent=create_acer_agent) # black blox
    # models are automatically saved



