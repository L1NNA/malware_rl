    #chainer good bc of ACER
#https://github.com/chainer/chainerrl
import numpy as np

import gym
import malware_rl





#creates an ACER agent
def create_acer_agent(env):
    #our observation space dimension of malware
    obs_dim = env.observation_space.shape[0]
    #the list of actions that we can perform on the malware
    n_actions = env.action_space.n
    #our acer network
    #consists of pi (our policy) and our q (our q function)
    model = acer.ACERSeparateModel(
        pi=links.Sequence(
            L.Linear( obs_dim, 1024, initialW=LeCunNormal(1e-3)),
            F.relu,
            L.Linear( 1024, 512, initialW=LeCunNormal(1e-3)),
            F.relu,
            L.Linear( 512, n_actions, initialW=LeCunNormal(1e-3)),
            SoftmaxDistribution),
        q=links.Sequence(
            L.Linear( obs_dim, 1024, initialW=LeCunNormal(1e-3)),
            F.relu,
            L.Linear( 1024, 512, initialW=LeCunNormal(1e-3)),
            F.relu,
            L.Linear( 512, n_actions, initialW=LeCunNormal(1e-3)),
            DiscreteActionValue),
        )
    #optimizer for the acer 
    opt = rmsprop_async.RMSpropAsync( lr=7e-4, eps=1e-2, alpha=0.99)
    opt.setup( model )
    #hook to the chainer model
    opt.add_hook( chainer.optimizer.GradientClipping(40) )

    replay_buffer = EpisodicReplayBuffer( 128 )
    #the agent itself, params from original file
    agent = acer.ACER( model, opt, 
        gamma=0.95, # reward discount factor
        t_max=32, # update the model after this many local steps
        replay_buffer=replay_buffer,
        n_times_replay=4, # number of times experience replay is repeated for each update
        replay_start_size=64, # don't start replay unless we have this many experiences in the buffer
        disable_online_update=True, # rely only on experience buffer
        use_trust_region=True,  # enable trust region policy optimiztion
        trust_region_delta=0.1,  # a parameter for TRPO
        truncation_threshold=5.0, # truncate large importance weights
        beta=1e-2, # entropy regularization parameter
        phi= lambda obs: obs.astype(np.float32, copy=False) )

    return agent





agent = create_acer_agent(env)
# pull latest stored model
last_model_dir = get_latest_model_from('models/acer_chainer')
agent.load( last_model_dir )



#training the ACER agent
def train_agent(rounds=10, use_score=False, name='result_dir', create_agent=create_acer_agent, gym_env = "malconv-train-v0"):
    #we are training on the malconv gym
    env = gym.make( gym_env ) 
    #setting random seeds so we can reproduce results
    np.random.seed(42)
    env.seed(42)
    #creating our agent
    agent = create_agent(env)
    
    chainerrl.experiments.collect_demonstrations(agent,env,steps = rounds,episodes = 1,outdir = name)
    
                 # Save everything to 'result' directory

    return agent